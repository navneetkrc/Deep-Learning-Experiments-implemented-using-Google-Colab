{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navneetkrc/Deep-Learning-Experiments-implemented-using-Google-Colab/blob/master/Build_a_Contextual_RAG_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install OpenAI, and LangChain dependencies"
      ],
      "metadata": {
        "id": "4vtFl39Ofu_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Contextual RAG System with Hybrid Search and Reranking\n",
        "\n",
        "![](https://i.imgur.com/BNeL4OZ.png)\n",
        "Based on Notebook by: [Dipanjan (DJ)](https://www.linkedin.com/in/dipanjans)___"
      ],
      "metadata": {
        "id": "jbw4wHV4zlKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.3.4\n",
        "!pip install langchain-openai==0.2.3\n",
        "!pip install langchain-community==0.3.3\n",
        "!pip install jq==1.8.0\n",
        "!pip install pymupdf==1.24.12\n",
        "!pip install httpx==0.27.2"
      ],
      "metadata": {
        "id": "LVX6450Lfu_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Chroma Vector DB LangChain wrapper"
      ],
      "metadata": {
        "id": "bwUBYHjPfu_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-chroma==0.1.4"
      ],
      "metadata": {
        "id": "p30SmCgTfu__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install BM25 dependencies"
      ],
      "metadata": {
        "id": "_3X1I_Vj8aU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25==0.2.2"
      ],
      "metadata": {
        "id": "xdp84Y9w6H1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Open AI API Key"
      ],
      "metadata": {
        "id": "EITC17hwfu__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691a784c-b6fb-4fc6-d5b6-39d0abdd9fb4",
        "id": "yEh2olNvfvAA"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Open AI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment Variables"
      ],
      "metadata": {
        "id": "pm_mx0v-fvAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
      ],
      "metadata": {
        "id": "Jhfb4gMUfvAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Processing the Data"
      ],
      "metadata": {
        "id": "afzeN_WkHIz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the dataset"
      ],
      "metadata": {
        "id": "RA_-hzHbFeSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if you can't download using the following code\n",
        "# go to https://drive.google.com/file/d/1aZxZejfteVuofISodUrY2CDoyuPLYDGZ download it\n",
        "# manually upload it on colab\n",
        "!gdown 1aZxZejfteVuofISodUrY2CDoyuPLYDGZ"
      ],
      "metadata": {
        "id": "RZFMYH-yFhWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip rag_docs.zip"
      ],
      "metadata": {
        "id": "WwLEBC4nF9ly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f6a1d5b-c3fe-4e10-c00c-cca1742f6bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  rag_docs.zip\n",
            "   creating: rag_docs/\n",
            "  inflating: rag_docs/attention_paper.pdf  \n",
            "  inflating: rag_docs/cnn_paper.pdf  \n",
            "  inflating: rag_docs/resnet_paper.pdf  \n",
            "  inflating: rag_docs/vision_transformer.pdf  \n",
            "  inflating: rag_docs/wikidata_rag_demo.jsonl  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Process JSON Documents"
      ],
      "metadata": {
        "id": "wMlxKZ_5jIdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import JSONLoader\n",
        "\n",
        "loader = JSONLoader(file_path='./rag_docs/wikidata_rag_demo.jsonl',\n",
        "                    jq_schema='.',\n",
        "                    text_content=False,\n",
        "                    json_lines=True)\n",
        "wiki_docs = loader.load()"
      ],
      "metadata": {
        "id": "RZ5y0NfzHPhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(wiki_docs)"
      ],
      "metadata": {
        "id": "G4E1zYFSG7J-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bc272e3-fc49-4fdb-b04a-180298496c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1801"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_docs[3]"
      ],
      "metadata": {
        "id": "aSbhERAyGw0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2933df03-3cb1-4b6a-fce5-3e10bb7db2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/rag_docs/wikidata_rag_demo.jsonl', 'seq_num': 4}, page_content='{\"id\": \"71548\", \"title\": \"Chi-square distribution\", \"paragraphs\": [\"In probability theory and statistics, the chi-square distribution (also chi-squared or formula_1\\\\u00a0 distribution) is one of the most widely used theoretical probability distributions. Chi-square distribution with formula_2 degrees of freedom is written as formula_3. It is a special case of gamma distribution.\", \"Chi-square distribution is primarily used in statistical significance tests and confidence intervals. It is useful, because it is relatively easy to show that certain probability distributions come close to it, under certain conditions. One of these conditions is that the null hypothesis must be true. Another one is that the different random variables (or observations) must be independent of each other.\"]}')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain.docstore.document import Document\n",
        "wiki_docs_processed = []\n",
        "\n",
        "for doc in wiki_docs:\n",
        "    doc = json.loads(doc.page_content)\n",
        "    metadata = {\n",
        "        \"title\": doc['title'],\n",
        "        \"id\": doc['id'],\n",
        "        \"source\": \"Wikipedia\",\n",
        "        \"page\": 1\n",
        "    }\n",
        "    data = ' '.join(doc['paragraphs'])\n",
        "    wiki_docs_processed.append(Document(page_content=data, metadata=metadata))"
      ],
      "metadata": {
        "id": "yICyAF85h2DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_docs_processed[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IATrHWKh7II",
        "outputId": "ab5cbbcb-85f1-4d6b-ebbc-cc0fe959a4b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'title': 'Chi-square distribution', 'id': '71548', 'source': 'Wikipedia', 'page': 1}, page_content='In probability theory and statistics, the chi-square distribution (also chi-squared or formula_1\\xa0 distribution) is one of the most widely used theoretical probability distributions. Chi-square distribution with formula_2 degrees of freedom is written as formula_3. It is a special case of gamma distribution. Chi-square distribution is primarily used in statistical significance tests and confidence intervals. It is useful, because it is relatively easy to show that certain probability distributions come close to it, under certain conditions. One of these conditions is that the null hypothesis must be true. Another one is that the different random variables (or observations) must be independent of each other.')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Process PDF documents"
      ],
      "metadata": {
        "id": "F_GzvHP1jSBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Chunk Contexts for Contextual Retrieval\n",
        "\n",
        "![](https://i.imgur.com/cqJqEv0.png)\n",
        "\n",
        "\n",
        "- Prepend chunk-specific explanatory context to each chunk before creating the vector DB embeddings and TF-IDF vectors.\n",
        "- Helps with having keywords or phrases in each chunk based on its relevance to the overall document.\n",
        "- Improves retrieval performance quite a bit, which also helps with the overall RAG generation results because of better context.\n",
        "- The contextual chunking prompt can be built in various ways depending on your use-case."
      ],
      "metadata": {
        "id": "4vH6xGFOnv7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ],
      "metadata": {
        "id": "jxHHyhlbl_9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create chunk context generation chain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "\n",
        "def generate_chunk_context(document, chunk):\n",
        "\n",
        "    chunk_process_prompt = \"\"\"You are an AI assistant specializing in research paper analysis.\n",
        "                            Your task is to provide brief, relevant context for a chunk of text\n",
        "                            based on the following research paper.\n",
        "\n",
        "                            Here is the research paper:\n",
        "                            <paper>\n",
        "                            {paper}\n",
        "                            </paper>\n",
        "\n",
        "                            Here is the chunk we want to situate within the whole document:\n",
        "                            <chunk>\n",
        "                            {chunk}\n",
        "                            </chunk>\n",
        "\n",
        "                            Provide a concise context (3-4 sentences max) for this chunk,\n",
        "                            considering the following guidelines:\n",
        "\n",
        "                            - Give a short succinct context to situate this chunk within the overall document\n",
        "                            for the purposes of improving search retrieval of the chunk.\n",
        "                            - Answer only with the succinct context and nothing else.\n",
        "                            - Context should be mentioned like 'Focuses on ....'\n",
        "                            do not mention 'this chunk or section focuses on...'\n",
        "\n",
        "                            Context:\n",
        "                        \"\"\"\n",
        "\n",
        "    prompt_template = ChatPromptTemplate.from_template(chunk_process_prompt)\n",
        "\n",
        "    agentic_chunk_chain = (prompt_template\n",
        "                                |\n",
        "                            chatgpt\n",
        "                                |\n",
        "                            StrOutputParser())\n",
        "\n",
        "    context = agentic_chunk_chain.invoke({'paper': document, 'chunk': chunk})\n",
        "\n",
        "    return context"
      ],
      "metadata": {
        "id": "MHSh0Vg-mIUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import uuid\n",
        "\n",
        "def create_contextual_chunks(file_path, chunk_size=3500, chunk_overlap=0):\n",
        "\n",
        "    print('Loading pages:', file_path)\n",
        "    loader = PyMuPDFLoader(file_path)\n",
        "    doc_pages = loader.load()\n",
        "\n",
        "    print('Chunking pages:', file_path)\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
        "                                              chunk_overlap=chunk_overlap)\n",
        "    doc_chunks = splitter.split_documents(doc_pages)\n",
        "\n",
        "    print('Generating contextual chunks:', file_path)\n",
        "    original_doc = '\\n'.join([doc.page_content for doc in doc_chunks])\n",
        "    contextual_chunks = []\n",
        "    for chunk in doc_chunks:\n",
        "        chunk_content = chunk.page_content\n",
        "        chunk_metadata = chunk.metadata\n",
        "        chunk_metadata_upd = {\n",
        "            'id': str(uuid.uuid4()),\n",
        "            'page': chunk_metadata['page'],\n",
        "            'source': chunk_metadata['source'],\n",
        "            'title': chunk_metadata['source'].split('/')[-1]\n",
        "        }\n",
        "        context = generate_chunk_context(original_doc, chunk_content)\n",
        "        contextual_chunks.append(Document(page_content=context+'\\n'+chunk_content,\n",
        "                                          metadata=chunk_metadata_upd))\n",
        "    print('Finished processing:', file_path)\n",
        "    print()\n",
        "    return contextual_chunks"
      ],
      "metadata": {
        "id": "-uxOSfcsxqHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "\n",
        "pdf_files = glob('./rag_docs/*.pdf')\n",
        "pdf_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-VUpdmczt0C",
        "outputId": "1f18af2a-fff8-4c05-b112-1a6b10aa5f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./rag_docs/resnet_paper.pdf',\n",
              " './rag_docs/cnn_paper.pdf',\n",
              " './rag_docs/vision_transformer.pdf',\n",
              " './rag_docs/attention_paper.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper_docs = []\n",
        "for fp in pdf_files:\n",
        "    paper_docs.extend(create_contextual_chunks(file_path=fp, chunk_size=3500))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsicBMPazzlg",
        "outputId": "acfd89c1-c0ea-451c-c533-75c3543f5593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pages: ./rag_docs/resnet_paper.pdf\n",
            "Chunking pages: ./rag_docs/resnet_paper.pdf\n",
            "Generating contextual chunks: ./rag_docs/resnet_paper.pdf\n",
            "Finished processing: ./rag_docs/resnet_paper.pdf\n",
            "\n",
            "Loading pages: ./rag_docs/cnn_paper.pdf\n",
            "Chunking pages: ./rag_docs/cnn_paper.pdf\n",
            "Generating contextual chunks: ./rag_docs/cnn_paper.pdf\n",
            "Finished processing: ./rag_docs/cnn_paper.pdf\n",
            "\n",
            "Loading pages: ./rag_docs/vision_transformer.pdf\n",
            "Chunking pages: ./rag_docs/vision_transformer.pdf\n",
            "Generating contextual chunks: ./rag_docs/vision_transformer.pdf\n",
            "Finished processing: ./rag_docs/vision_transformer.pdf\n",
            "\n",
            "Loading pages: ./rag_docs/attention_paper.pdf\n",
            "Chunking pages: ./rag_docs/attention_paper.pdf\n",
            "Generating contextual chunks: ./rag_docs/attention_paper.pdf\n",
            "Finished processing: ./rag_docs/attention_paper.pdf\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(paper_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOkwPMxp0gFh",
        "outputId": "6c0563e1-f4f2-4382-f1f9-210efcc6dd75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper_docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVK2i0lR0ieV",
        "outputId": "099e3a63-0c93-4734-81fc-45356bd93021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'id': 'd5c90113-2421-42c0-bf09-813faaf75ac7', 'page': 0, 'source': './rag_docs/resnet_paper.pdf', 'title': 'resnet_paper.pdf'}, page_content='Focuses on the introduction of a residual learning framework designed to facilitate the training of significantly deeper neural networks, addressing challenges such as vanishing gradients and degradation of accuracy. It highlights the empirical success of residual networks, particularly their performance on the ImageNet dataset and their foundational role in winning multiple competitions in 2015.\\nDeep Residual Learning for Image Recognition\\nKaiming He\\nXiangyu Zhang\\nShaoqing Ren\\nJian Sun\\nMicrosoft Research\\n{kahe, v-xiangz, v-shren, jiansun}@microsoft.com\\nAbstract\\nDeeper neural networks are more difﬁcult to train. We\\npresent a residual learning framework to ease the training\\nof networks that are substantially deeper than those used\\npreviously. We explicitly reformulate the layers as learn-\\ning residual functions with reference to the layer inputs, in-\\nstead of learning unreferenced functions. We provide com-\\nprehensive empirical evidence showing that these residual\\nnetworks are easier to optimize, and can gain accuracy from\\nconsiderably increased depth. On the ImageNet dataset we\\nevaluate residual nets with a depth of up to 152 layers—8×\\ndeeper than VGG nets [41] but still having lower complex-\\nity. An ensemble of these residual nets achieves 3.57% error\\non the ImageNet test set. This result won the 1st place on the\\nILSVRC 2015 classiﬁcation task. We also present analysis\\non CIFAR-10 with 100 and 1000 layers.\\nThe depth of representations is of central importance\\nfor many visual recognition tasks. Solely due to our ex-\\ntremely deep representations, we obtain a 28% relative im-\\nprovement on the COCO object detection dataset. Deep\\nresidual nets are foundations of our submissions to ILSVRC\\n& COCO 2015 competitions1, where we also won the 1st\\nplaces on the tasks of ImageNet detection, ImageNet local-\\nization, COCO detection, and COCO segmentation.\\n1. Introduction\\nDeep convolutional neural networks [22, 21] have led\\nto a series of breakthroughs for image classiﬁcation [21,\\n50, 40]. Deep networks naturally integrate low/mid/high-\\nlevel features [50] and classiﬁers in an end-to-end multi-\\nlayer fashion, and the “levels” of features can be enriched\\nby the number of stacked layers (depth). Recent evidence\\n[41, 44] reveals that network depth is of crucial importance,\\nand the leading results [41, 44, 13, 16] on the challenging\\nImageNet dataset [36] all exploit “very deep” [41] models,\\nwith a depth of sixteen [41] to thirty [16]. Many other non-\\ntrivial visual recognition tasks [8, 12, 7, 32, 27] have also\\n1http://image-net.org/challenges/LSVRC/2015/\\nand\\nhttp://mscoco.org/dataset/#detections-challenge2015.\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0 \\n10\\n20\\niter. (1e4)\\ntraining error (%)\\n \\n \\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n10\\n20\\niter. (1e4)\\ntest error (%)\\n \\n \\n56-layer\\n20-layer\\n56-layer\\n20-layer\\nFigure 1. Training error (left) and test error (right) on CIFAR-10\\nwith 20-layer and 56-layer “plain” networks. The deeper network\\nhas higher training error, and thus test error. Similar phenomena\\non ImageNet is presented in Fig. 4.\\ngreatly beneﬁted from very deep models.\\nDriven by the signiﬁcance of depth, a question arises: Is\\nlearning better networks as easy as stacking more layers?\\nAn obstacle to answering this question was the notorious\\nproblem of vanishing/exploding gradients [1, 9], which\\nhamper convergence from the beginning.\\nThis problem,\\nhowever, has been largely addressed by normalized initial-\\nization [23, 9, 37, 13] and intermediate normalization layers\\n[16], which enable networks with tens of layers to start con-\\nverging for stochastic gradient descent (SGD) with back-\\npropagation [22].\\nWhen deeper networks are able to start converging, a\\ndegradation problem has been exposed: with the network\\ndepth increasing, accuracy gets saturated (which might be\\nunsurprising) and then degrades rapidly.\\nUnexpectedly,\\nsuch degradation is not caused by overﬁtting, and adding')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine all document chunks in one list"
      ],
      "metadata": {
        "id": "UyPdlZo2xEly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(wiki_docs_processed)"
      ],
      "metadata": {
        "id": "UbtpR-r50mEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "588844a6-f601-4d13-8607-3b8973866391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1801"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_docs = wiki_docs_processed + paper_docs\n",
        "len(total_docs)"
      ],
      "metadata": {
        "id": "lNQWgq9t0pMH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c3d474-8ff8-423d-851c-0e5a42188346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1880"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Index Document Chunks and Embeddings in Vector DB\n",
        "\n",
        "Here we initialize a connection to a Chroma vector DB client, and also we want to save to disk, so we simply initialize the Chroma client and pass the directory where we want the data to be saved to."
      ],
      "metadata": {
        "id": "Daqn6Hglw9Nk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Open AI Embedding Models\n",
        "\n",
        "LangChain enables us to access Open AI embedding models which include the newest models: a smaller and highly efficient `text-embedding-3-small` model, and a larger and more powerful `text-embedding-3-large` model."
      ],
      "metadata": {
        "id": "jiokYxD8fvAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
        "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
      ],
      "metadata": {
        "id": "-On4AS0HfvAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector DB Indexing for Semantic Search"
      ],
      "metadata": {
        "id": "wSbIwM4s5VBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# create vector DB of docs and embeddings - takes < 30s on Colab\n",
        "chroma_db = Chroma.from_documents(documents=total_docs,\n",
        "                                  collection_name='my_context_db',\n",
        "                                  embedding=openai_embed_model,\n",
        "                                  # need to set the distance function to cosine else it uses euclidean by default\n",
        "                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
        "                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                  persist_directory=\"./my_context_db\")"
      ],
      "metadata": {
        "id": "ZhAQyrFBfvAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ju_zBIj1Zsb"
      },
      "source": [
        "### Load Vector DB from disk\n",
        "\n",
        "This is just to show once you have a vector database on disk you can just load and create a connection to it anytime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNvj0dDH1WDg"
      },
      "outputs": [],
      "source": [
        "# load from disk\n",
        "chroma_db = Chroma(persist_directory=\"./my_context_db\",\n",
        "                   collection_name='my_context_db',\n",
        "                   embedding_function=openai_embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFC3uPqYop0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b1a38a6-9f7b-4ece-fb66-db4310ab55d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_chroma.vectorstores.Chroma at 0x7e84015b1330>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "chroma_db"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Semantic Similarity based Retrieval\n",
        "\n",
        "We use simple cosine similarity here and retrieve the top 5 similar documents based on the user input query"
      ],
      "metadata": {
        "id": "njfZOOVZxj1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
        "                                              search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "tV1l6HYdxj1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BM25 Indexing for Keyword based Retrieval"
      ],
      "metadata": {
        "id": "gsn6HfgA5Yc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(documents=total_docs,\n",
        "                                              k=5)\n",
        "bm25_retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQKePy925MvI",
        "outputId": "71b6092b-7708-4186-fab4-c91c7976edf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x7e84015b33a0>, k=5)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Retrieval Strategy"
      ],
      "metadata": {
        "id": "zxa3xf_D6SYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Base Ensemble Retriever"
      ],
      "metadata": {
        "id": "5AetRdtr6W_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "# reciprocal rank fusion\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, similarity_retriever],\n",
        "    weights=[0.5, 0.5]\n",
        ")\n",
        "ensemble_retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "738cpSm_5SQ5",
        "outputId": "891eae01-af45-4fde-e877-46c87a429159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EnsembleRetriever(retrievers=[BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x7e84015b33a0>, k=5), VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7e84015b1330>, search_kwargs={'k': 5})], weights=[0.5, 0.5])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chained Retrieval with Reranker"
      ],
      "metadata": {
        "id": "DteKLQTM7EnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "\n",
        "# download an open-source reranker model - BAAI/bge-reranker-v2-m3\n",
        "reranker = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
        "reranker_compressor = CrossEncoderReranker(model=reranker, top_n=5)\n",
        "# Retriever 2 - Uses a Reranker model to rerank retrieval results from the previous retriever\n",
        "final_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker_compressor,\n",
        "    base_retriever=ensemble_retriever\n",
        ")\n",
        "final_retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhnRVwst7D_5",
        "outputId": "2d0b77a2-42d7-4d02-993b-ff8a1f52465c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ContextualCompressionRetriever(base_compressor=CrossEncoderReranker(model=HuggingFaceCrossEncoder(client=<sentence_transformers.cross_encoder.CrossEncoder.CrossEncoder object at 0x7e82f6796710>, model_name='BAAI/bge-reranker-v2-m3', model_kwargs={}), top_n=5), base_retriever=EnsembleRetriever(retrievers=[BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x7e84015b33a0>, k=5), VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7e84015b1330>, search_kwargs={'k': 5})], weights=[0.5, 0.5]))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def display_docs(docs):\n",
        "    for doc in docs:\n",
        "        print('Metadata:', doc.metadata)\n",
        "        print('Content Brief:')\n",
        "        display(Markdown(doc.page_content[:1000]))\n",
        "        print()"
      ],
      "metadata": {
        "id": "nUIJG_bDxj1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is machine learning?\"\n",
        "top_docs = final_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "id": "PIh4xGv2xj1c",
        "outputId": "0018ffe9-f288-401d-e097-e71d25eeecc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '564928', 'page': 1, 'source': 'Wikipedia', 'title': 'Machine learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '663523', 'page': 1, 'source': 'Wikipedia', 'title': 'Deep learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, whic"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '359370', 'page': 1, 'source': 'Wikipedia', 'title': 'Supervised learning'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '44742', 'page': 1, 'source': 'Wikipedia', 'title': 'Artificial neural network'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': 'fc4c7a16-11e3-4dd4-9e5f-8152dc481836', 'page': 0, 'source': './rag_docs/cnn_paper.pdf', 'title': 'cnn_paper.pdf'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the introduction of Convolutional Neural Networks (CNNs) within the broader field of Artificial Neural Networks (ANNs), highlighting their significance in image-driven pattern recognition tasks. It outlines the foundational concepts of ANNs, their architecture, and the evolution of machine learning techniques, setting the stage for a deeper exploration of CNNs and their applications.\nAn Introduction to Convolutional Neural Networks\nKeiron O’Shea1 and Ryan Nash2\n1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB\nkeo7@aber.ac.uk\n2 School of Computing and Communications, Lancaster University, Lancashire, LA1\n4YW\nnashrd@live.lancs.ac.uk\nAbstract. The ﬁeld of machine learning has taken a dramatic twist in re-\ncent times, with the rise of the Artiﬁcial Neural Network (ANN). These\nbiologically inspired computational models are able to far exceed the per-\nformance of previous forms of artiﬁcial intelligence in common machine\nlearning tasks. One of the mos"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is the difference between transformers and vision transformers?\"\n",
        "top_docs = final_retriever.invoke(query)\n",
        "display_docs(top_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S_PXFMcJxuyO",
        "outputId": "70d9cd57-de44-4f8e-f880-3cd05c26d2b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata: {'id': '634ee903-03bc-42d1-ba48-615254cf9820', 'page': 0, 'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the introduction of the Vision Transformer (ViT) model, which applies a pure Transformer architecture to image classification tasks by treating image patches as tokens. It highlights the limitations of traditional convolutional networks in computer vision and presents the advantages of using Transformers, particularly when pre-trained on large datasets, achieving competitive results with reduced computational resources.\nPublished as a conference paper at ICLR 2021\nAN IMAGE IS WORTH 16X16 WORDS:\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\nAlexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\nXiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\n∗equal technical contribution, †equal advising\nGoogle Research, Brain Team\n{adosovitskiy, neilhoulsby}@google.com\nABSTRACT\nWhile the Transformer architecture has become the de-facto standard for natural\nlanguage processing tasks, i"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': 'b896c93d-6330-421c-a236-af9437e9c725', 'page': 1, 'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the performance of the Vision Transformer (ViT) in comparison to convolutional neural networks (CNNs), highlighting the advantages of large-scale training on datasets like ImageNet-21k and JFT-300M. It discusses how ViT achieves state-of-the-art results in image recognition benchmarks despite lacking certain inductive biases inherent to CNNs. Additionally, it references related work on self-attention mechanisms and their application in computer vision.\nPublished as a conference paper at ICLR 2021\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\nwhen trained on insufﬁcient amounts of data.\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We\nﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\nresults when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\npre-trained on the public ImageNet-21k dataset "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': '45cb4930-8e97-42f7-a1e8-17d525f25118', 'page': 7, 'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on a controlled scaling study of various models, including Vision Transformers and ResNets, evaluating their transfer performance from the JFT-300M dataset. It highlights the performance versus pre-training cost, revealing that Vision Transformers generally outperform ResNets in terms of efficiency and scalability, while also discussing the implications for future model scaling efforts.\nPublished as a conference paper at ICLR 2021\n4.4\nSCALING STUDY\nWe perform a controlled scaling study of different models by evaluating transfer performance from\nJFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': 'd3a31c54-e58a-4b69-92f4-6cd180c3377d', 'page': 2, 'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the architecture and methodology of the Vision Transformer (ViT), detailing how images are processed by splitting them into patches, embedding these patches, and utilizing a standard Transformer encoder for image classification tasks. It also describes the integration of position embeddings and the classification head within the model framework.\nPublished as a conference paper at ICLR 2021\nTransformer Encoder\nMLP \nHead\nVision Transformer (ViT)\n*\nLinear Projection of Flattened Patches\n* Extra learnable\n     [ cl ass]  embedding\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\nPatch + Position \nEmbedding\nClass\nBird\nBall\nCar\n...\nEmbedded \nPatches\nMulti-Head \nAttention\nNorm\nMLP\nNorm\n+\nL x\n+\nTransformer Encoder\nFigure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\nencoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable\n“classiﬁc"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metadata: {'id': 'fcceed86-4540-40e9-9a64-cdec9ae0da4f', 'page': 7, 'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n",
            "Content Brief:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Focuses on the behavior of attention mechanisms in the Vision Transformer (ViT) architecture, highlighting how attention distances vary across layers and the implications for feature extraction. It compares the attention patterns in ViT to those in hybrid models that incorporate convolutional networks, suggesting that early layers in ViT exhibit localized attention similar to CNNs. Additionally, it notes the increase in attention distance with deeper layers, indicating a shift towards more global feature integration relevant for classification tasks.\nhave consistently small attention distances in the low layers. This highly localized attention is\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\nsuggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\nattention distance increases with network depth. Globally, we ﬁnd that the model attends to image\nregions that are semantically relevant for classiﬁca"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the RAG Pipeline"
      ],
      "metadata": {
        "id": "gQFWv7YUyVII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n",
        "                Answer the following question using only the following pieces of retrieved context.\n",
        "                If the answer is not in the context, do not make up answers, just say that you don't know.\n",
        "                Keep the answer detailed and well formatted based on the information from the context.\n",
        "\n",
        "                Question:\n",
        "                {question}\n",
        "\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Answer:\n",
        "            \"\"\"\n",
        "\n",
        "rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)"
      ],
      "metadata": {
        "id": "PHOrfGXKyVIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "qa_rag_chain = (\n",
        "    {\n",
        "        \"context\": (final_retriever\n",
        "                      |\n",
        "                    format_docs),\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "      |\n",
        "    rag_prompt_template\n",
        "      |\n",
        "    chatgpt\n",
        ")"
      ],
      "metadata": {
        "id": "KmWeCB4yyVIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "query = \"What is machine learning?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "ac37fca9-0e57-4388-e349-7c50092ac208",
        "id": "xvj_eGIWyVIJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Machine learning is a subfield of computer science that provides computers the ability to learn without being explicitly programmed. The concept was introduced by Arthur Samuel in 1959 and is rooted in artificial intelligence. Machine learning focuses on the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. These algorithms follow programmed instructions but can also adapt and improve their performance by building models from sample inputs.\n\nMachine learning is particularly useful in scenarios where designing and programming explicit algorithms is impractical. Some common applications of machine learning include:\n\n- Spam filtering\n- Detection of network intruders or malicious insiders\n- Optical character recognition (OCR)\n- Search engines\n- Computer vision\n\nWithin the realm of machine learning, there is a subset known as deep learning, which primarily utilizes certain types of neural networks. Deep learning involves learning sessions that can be unsupervised, semi-supervised, or supervised, and it often includes multiple layers of processing, allowing the model to learn increasingly abstract representations of the data.\n\nOverall, machine learning represents a significant advancement in the ability of computers to process information and make informed decisions based on that information."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is a CNN?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "pXtezDlZzadt",
        "outputId": "0a930c29-121a-4602-9322-f8d609f6738a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A Convolutional Neural Network (CNN) is a specialized type of Artificial Neural Network (ANN) that is particularly effective for image-driven pattern recognition tasks. CNNs are designed to process data with a grid-like topology, such as images, and they utilize a unique architecture that distinguishes them from traditional ANNs.\n\n### Key Features of CNNs:\n\n1. **Three-Dimensional Neuron Organization**: \n   - In CNNs, neurons are organized in three dimensions: height, width, and depth. This structure allows CNNs to capture spatial hierarchies in images, where the depth dimension corresponds to the number of channels (e.g., RGB for color images).\n\n2. **Layer Types**:\n   - CNNs are composed of three main types of layers:\n     - **Convolutional Layers**: These layers apply convolution operations to the input, allowing the network to learn spatial hierarchies and features from the images. Each neuron in a convolutional layer connects to a small region of the input, which helps in detecting local patterns.\n     - **Pooling Layers**: These layers reduce the spatial dimensions of the input, helping to decrease the computational load and control overfitting by summarizing the features.\n     - **Fully-Connected Layers**: These layers connect every neuron in one layer to every neuron in the next layer, typically used at the end of the network to produce the final output.\n\n3. **Functionality**:\n   - The input layer holds the pixel values of the image, while the convolutional layers compute outputs based on local regions of the input. The pooling layers then condense these outputs, and the fully-connected layers produce the final classification scores.\n\n4. **Learning Paradigms**:\n   - CNNs primarily utilize supervised learning, where the model is trained on labeled data to minimize classification errors. This is crucial for tasks such as image classification, where the goal is to correctly identify the class of an input image.\n\n5. **Architectural Design**:\n   - A common practice in CNN architecture is to stack multiple convolutional layers before pooling layers. This enhances feature extraction capabilities and allows the network to learn more complex patterns.\n\n6. **Resource Intensity**:\n   - CNNs can be resource-intensive, especially when processing large images. Techniques such as zero-padding and careful management of layer dimensions are often employed to optimize performance and memory usage.\n\nIn summary, CNNs are a powerful tool in the field of machine learning, particularly for image analysis and pattern recognition, leveraging their unique architecture to effectively learn and classify visual data."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How is a resnet better than a CNN?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "Fo92-ZmIELPF",
        "outputId": "d902450f-a3c2-4865-e354-b1e085f65e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A ResNet (Residual Network) is considered better than a traditional CNN (Convolutional Neural Network) for several reasons, particularly in the context of training deeper architectures and achieving better performance in various tasks. Here are the key advantages of ResNets over standard CNNs:\n\n1. **Degradation Problem Mitigation**: Traditional CNNs often face the degradation problem, where increasing the depth of the network leads to higher training error. ResNets address this issue by introducing shortcut connections that allow gradients to flow more easily during backpropagation. This makes it easier to optimize deeper networks, as the residual learning framework allows the model to learn residual mappings instead of the original unreferenced mappings.\n\n2. **Higher Accuracy with Increased Depth**: ResNets can be significantly deeper than traditional CNNs without suffering from performance degradation. For instance, ResNet architectures with 50, 101, or even 152 layers have been shown to achieve better accuracy compared to shallower networks. The empirical results demonstrate that deeper ResNets can produce substantially better results on datasets like ImageNet and CIFAR-10.\n\n3. **Generalization Performance**: ResNets exhibit good generalization performance across various recognition tasks. The context mentions that replacing VGG-16 with ResNet-101 in the Faster R-CNN framework led to a notable increase in detection metrics on challenging datasets like COCO, indicating that ResNets can generalize better to unseen data.\n\n4. **Architectural Efficiency**: Despite being deeper, ResNets maintain lower computational complexity compared to traditional architectures like VGG-16. For example, a 152-layer ResNet has lower complexity (11.3 billion FLOPs) than VGG-16 (15.3 billion FLOPs), allowing for more efficient training and inference.\n\n5. **Empirical Success in Competitions**: ResNets have achieved top rankings in various competitions, such as ILSVRC and COCO 2015, demonstrating their effectiveness in real-world applications. The context highlights that models based on deep residual networks won first places in several tracks, showcasing their superior performance.\n\nIn summary, ResNets improve upon traditional CNNs by effectively addressing the degradation problem, enabling deeper architectures to be trained successfully, achieving higher accuracy, and demonstrating strong generalization capabilities across different tasks."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is NLP and its relation to linguistics?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "J5IQoBc0zlAr",
        "outputId": "a0a1d249-bf07-4171-c352-bcadfa4aa9f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Natural Language Processing (NLP) is a field within Artificial Intelligence that focuses on the interaction between computers and human languages. The primary goal of NLP is to enable computers to automatically understand, interpret, and generate human languages, which includes both writing and speaking. The term \"Natural Language\" specifically refers to human languages, distinguishing them from programming languages used in computing.\n\nNLP is closely related to linguistics, which is the scientific study of language and its structure. Linguistics provides the foundational theories and frameworks that inform NLP techniques, as understanding the nuances of human language—such as syntax, semantics, and pragmatics—is essential for developing effective NLP applications. By leveraging insights from linguistics, NLP aims to create systems that can process and analyze large amounts of natural language data, facilitating tasks such as translation, sentiment analysis, and conversational agents. \n\nIn summary, NLP is a crucial intersection of artificial intelligence and linguistics, aiming to bridge the gap between human communication and machine understanding."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the difference between AI, ML and DL?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "AzeZuG1hzvGy",
        "outputId": "5c4b35ed-0ed8-4f84-d847-4135d6ad839e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The difference between AI, ML, and DL can be summarized as follows:\n\n### Artificial Intelligence (AI)\n- **Definition**: AI refers to the ability of a computer program or machine to think and learn, mimicking human cognition. It encompasses a broad range of technologies and applications aimed at making machines \"smart.\"\n- **Origin**: The term \"Artificial Intelligence\" was coined by John McCarthy in 1955.\n- **Functionality**: AI systems can interpret external data, learn from it, and adapt to achieve specific goals. As technology advances, tasks once considered to require intelligence, like optical character recognition, are no longer classified as AI.\n\n### Machine Learning (ML)\n- **Definition**: ML is a subfield of AI that focuses on the development of algorithms that allow computers to learn from and make predictions based on data without being explicitly programmed.\n- **Functionality**: ML algorithms build models from sample inputs and can make decisions or predictions based on data. It is particularly useful in scenarios where traditional programming is impractical, such as spam filtering and computer vision.\n\n### Deep Learning (DL)\n- **Definition**: DL is a specialized subset of machine learning that primarily uses neural networks with multiple layers (multi-layer neural networks) to process data.\n- **Functionality**: In deep learning, the information processed becomes increasingly abstract with each added layer, making it particularly effective for complex tasks like speech and image recognition. DL models are inspired by the biological nervous system but differ significantly from the structural and functional properties of human brains.\n\nIn summary, AI is the overarching field that includes both ML and DL, with ML being a specific approach within AI that enables learning from data, and DL being a further specialization of ML that utilizes deep neural networks for more complex data processing tasks."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the difference between transformers and vision transformers?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "cWihDiL3zPzY",
        "outputId": "b3613586-3a9c-420b-9a1f-6a815962b6a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The primary difference between traditional Transformers and Vision Transformers (ViT) lies in their application and input processing methods.\n\n1. **Input Representation**:\n   - **Transformers**: In natural language processing (NLP), Transformers operate on sequences of tokens (words) that are typically represented as embeddings. The input is a 1D sequence of these token embeddings.\n   - **Vision Transformers (ViT)**: ViT adapts the Transformer architecture for image classification tasks by treating image patches as tokens. An image is divided into fixed-size patches, which are then flattened and linearly embedded into a sequence. This sequence of patch embeddings is fed into the Transformer, similar to how word embeddings are processed in NLP.\n\n2. **Architecture**:\n   - **Transformers**: The standard Transformer architecture consists of layers of multi-headed self-attention and feed-forward neural networks, designed to capture relationships and dependencies in sequential data.\n   - **Vision Transformers (ViT)**: While ViT retains the core Transformer architecture, it modifies the input to accommodate 2D image data. The model includes additional components such as position embeddings to retain spatial information about the patches, which is crucial for understanding the structure of images.\n\n3. **Performance and Efficiency**:\n   - **Transformers**: In NLP, Transformers have become the standard due to their ability to scale and perform well on large datasets, often requiring significant computational resources.\n   - **Vision Transformers (ViT)**: ViT has shown that a pure Transformer can achieve competitive results in image classification, often outperforming traditional convolutional neural networks (CNNs) in terms of efficiency and scalability when pre-trained on large datasets. ViT requires substantially fewer computational resources to train compared to state-of-the-art CNNs, making it a promising alternative for image recognition tasks.\n\nIn summary, while both architectures utilize the Transformer framework, Vision Transformers adapt the input and processing methods to effectively handle image data, demonstrating significant advantages in performance and resource efficiency in the realm of computer vision."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How is self-attention important in transformers?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "k1lqzejlEvsj",
        "outputId": "fe19593a-386e-4706-a87b-f2de0838548b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Self-attention is crucial in transformers for several reasons:\n\n1. **Parallelization**: Self-attention mechanisms allow for significant parallelization of computations. Unlike recurrent layers, which require sequential processing, self-attention can process all input positions simultaneously. This leads to faster training times and more efficient use of computational resources.\n\n2. **Learning Long-Range Dependencies**: One of the key advantages of self-attention is its ability to learn long-range dependencies within the input data. In traditional recurrent neural networks (RNNs), the path length between input and output positions can be long, making it difficult to learn relationships between distant elements. Self-attention, on the other hand, connects all positions with a constant number of sequential operations, facilitating the learning of these long-range dependencies.\n\n3. **Computational Complexity**: The computational complexity of self-attention is O(n² · d), where n is the sequence length and d is the representation dimension. This is more efficient than recurrent layers, which have a complexity of O(n · d²). This efficiency becomes particularly beneficial as the sequence length increases.\n\n4. **Flexibility in Input Representation**: Self-attention allows for flexible input representations, as it can weigh the importance of different parts of the input dynamically. This is particularly useful in tasks where the relevance of input elements can vary significantly.\n\n5. **Application in Vision Transformers**: In the context of Vision Transformers (ViT), self-attention mechanisms have been adapted to process images by applying attention to patches of the image rather than individual pixels. This adaptation allows transformers to achieve state-of-the-art results in image recognition tasks, demonstrating the versatility and effectiveness of self-attention beyond traditional NLP applications.\n\nOverall, self-attention enhances the transformer's ability to process data efficiently and effectively, making it a foundational component of the architecture."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How does a resnet work?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "7Zf_BjmlFBcb",
        "outputId": "f33927b3-cd52-46f0-8879-9162daf4a3c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A ResNet, or Residual Network, operates on the principle of residual learning to address the challenges associated with training deep neural networks. Here’s a detailed explanation of how it works:\n\n### Key Concepts of ResNet\n\n1. **Residual Mapping**:\n   - Instead of learning the desired underlying mapping \\( H(x) \\) directly, ResNets focus on learning a residual mapping \\( F(x) = H(x) - x \\). This means that the network learns the difference between the desired output and the input, which is often easier to optimize.\n\n2. **Shortcut Connections**:\n   - ResNets utilize shortcut connections that skip one or more layers. These connections perform identity mapping, allowing the input \\( x \\) to be added directly to the output of the stacked layers. This can be mathematically represented as:\n     \\[\n     H(x) = F(x) + x\n     \\]\n   - The addition of the input \\( x \\) helps in mitigating the vanishing gradient problem, making it easier for the network to learn.\n\n3. **Optimization Benefits**:\n   - The formulation of \\( F(x) + x \\) allows the network to push the residual \\( F(x) \\) towards zero if the identity mapping is optimal. This is generally easier than fitting a complex mapping directly, especially as the depth of the network increases.\n\n### Architecture\n\n- ResNets can be constructed with various depths, such as 18, 34, 50, 101, and even 152 layers. The architecture includes:\n  - **Convolutional Layers**: These layers extract features from the input images.\n  - **Batch Normalization**: Applied after each convolution to stabilize and accelerate training.\n  - **Pooling Layers**: Used for down-sampling the feature maps.\n  - **Fully Connected Layers**: At the end of the network for classification tasks.\n\n### Performance\n\n- ResNets have shown significant improvements in accuracy as the depth increases, unlike traditional plain networks, which suffer from higher training errors with increased depth. For instance, a 34-layer ResNet outperforms an 18-layer ResNet, demonstrating that deeper networks can be effectively trained without degradation in performance.\n\n### Empirical Results\n\n- Extensive experiments on datasets like ImageNet and CIFAR-10 have validated the effectiveness of ResNets. They have achieved state-of-the-art results, including winning the ILSVRC 2015 competition with a 152-layer ResNet, which had lower complexity than previous models like VGG-16/19.\n\nIn summary, ResNets leverage residual learning and shortcut connections to facilitate the training of very deep networks, overcoming the optimization difficulties that typically arise with increased depth. This architecture has proven to be highly effective in various image recognition tasks."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is LangChain?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "Ttz53mEy0J_D",
        "outputId": "44e1f3bb-5383-44d4-d81e-0db7a3781160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I don't know."
          },
          "metadata": {}
        }
      ]
    }
  ]
}